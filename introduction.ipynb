{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g89rPGiUzU0"
   },
   "source": [
    "#### 기존 LLM (ChatGPT)의 한계점\n",
    "##### ChatGPT는 최신 정보가 학습되어 있지 않습니다.\n",
    "##### 개인이나 회사의 내부 데이터가 학습되어 있지 않아, 특정 도메인(개인 정보, 회사 내부 정보)에 대한 질문에는 기대하는 답변을 얻을 수 없습니다.\n",
    "##### ChatGPT에 개인이나 회사 정보를 담은 문서를 업로드하면 보안상 문제가 될 수 있습니다.\n",
    "##### 문서의 양이 많아질수록 할루시네이션 현상이 발생하기 쉽습니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NkSvj_GVoNe"
   },
   "source": [
    "#### RAG (Retriever-Augmented Generation)\n",
    "##### RAG는 LLM이 외부의 신뢰할 수 있는 지식 데이터베이스를 참조하여 최적화된 응답을 생성하는 기술입니다.\n",
    "##### 최신 정보를 기반으로 답변할 수 있으며, LLM이 정보를 찾을 수 없는 경우 '검색' 기능을 활용해 답변을 제공할 수 있습니다.\n",
    "##### 회사 내부에 데이터베이스를 구현함으로써 개인이나 회사의 내부 데이터를 참고하여 GPT가 답변할 수 있습니다.\n",
    "##### 문서를 내부 데이터베이스에 저장하고 지속적으로 데이터를 축적할 수 있으며, 저장된 데이터베이스에서 원하는 정보를 검색한 후 이를 바탕으로 답변을 생성할 수 있습니다.\n",
    "##### 저장된 데이터베이스에서 답변의 출처를 역으로 검색하고 검증하는 방식으로 할루시네이션 현상을 줄일 수 있습니다.\n",
    "##### 따라서 RAG를 사용하면 LLM이 사전 학습한 내용뿐 아니라 새롭게 축적되는 데이터베이스를 기반으로 답변할 수 있어, 사용자는 데이터베이슴나 업데이트하면 더 높은 품질의 최신 답변을 얻을 수 있습니다.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4uexMKBV1ne"
   },
   "source": [
    "#### LangChain\n",
    "##### LangChain은 대규모 언어 모델로 구동되는 애플리케이션을 개발하기 위한 프레임워크입니다.\n",
    "##### 즉, GPT와 같은 언어 모델과 우리가 만들고자 하는 서비스나 프로세스를 쉽게 연결해 주는 도구입니다.\n",
    "##### ChatGPT는 자체 RAG 시스템을 통해 답변을 제공하지만, 더 나은 답변을 위해 세부 알고리즘을 조정할 수 없다는 문제가 있습니다.\n",
    "##### 따라서 LangChain을 통해 RAG의 모든 세부 프로세스를 빠르고 쉽게 구현하면 다양한 비즈니스 환경에서도 높은 수준의 성능에 도달할 수 있습니다.\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tRLwkH8aWf1R"
   },
   "source": [
    "#### RAG 프로세스\n",
    "\n",
    "#### 사전 단계\n",
    "##### 1단계 : 문서 로드 (Document Load) : 외부 데이터 소스에서 필요한 문서를 불러와서 초기 처리를 합니다. 이것은 마치 학생이 공부하기 전에 책장에서 필요한 책을 여러 권 챙겨 오는 과정과 같습니다.\n",
    "##### 2단계 : 텍스트 분할 (Text Split) : 로드된 문서를 처리 가능한 작은 단위인 청크(chunk)로 분할합니다. 두꺼운 책을 주제별로 나누어 Part나 Chapter로 구분하는 것과 유사합니다. 보통 분할된 청크 끝부분에서 맥락이 이어질 수 있도록 일부를 겹쳐서 분할합니다. (Chunk Overlap)\n",
    "##### 3단계 : 임베딩 (Embedding) : 분할된 청크를 벡터 형태로 변환하여 문서의 의미를 수치화합니다. 자연어를 컴퓨터가 이해할 수 있는 수치로 변경하는 과정입니다. (임베딩 차원이 커지면 유사도 비교 정확도는 좋아지지만 연산 및 저장 비용이 증가)\n",
    "##### 4단계 : 벡터 스토어 저장 : 임베딩된 청크를 데이터베이스에 저장합니다. (LLM 요청 시마다 임베딩하면 비요이 계속 발생하기 때문) 이는 요약된 키워드를 색인으로 뽑아서 나중에 빠르게 찾을 수 있게 정리해 두는 과정입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrUopKyiXGqn"
   },
   "source": [
    "#### RAG 프로세스 실행 단계\n",
    "##### 5단계 : 리트리버 (Retreiver) : 질문이 주어지면, 이와 관련된 벡터를 벡터 데이터베이스에서 검색합니다.(동일하게 질문을 먼저 임베딩한 후 두 벡터간의 유사도 비교) 질문에 가장 잘 맞는 책의 Chapter를 찾는 것과 유사합니다.\n",
    "##### 6단계 : 프롬프트 (Prompt) : 검색된 정보를 바탕으로 언어 모델을 위한 질문을 구성합니다. 이는 정보를 바탕으로 어떻게 질문할지 결정하는 과정입니다. 잘 구성된 프롬프트는 모델이 더 정확하고 관련성 높은 답변을 만들어 낼 수 있는 중요한 토대가 됩니다.\n",
    "##### 7단계 : LLM : 구성된 프롬프트를 사용하여 언어 모델이 답변을 생성합니다. 즉, 수집된 정보를 바탕으로 과제나 보고서를 작성하는 학생과 같습니다.\n",
    "##### 8단계 : 체인 생성 : LCEL (Langchain Expression Language) 문법을 활용해 이전의 모든 과정을 하나의 파이프라인으로 묶어 주는 체인(Chain)을 생성합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MelxNst0XOR_"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNguFBfKktP7g0PWnIDBf23",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
